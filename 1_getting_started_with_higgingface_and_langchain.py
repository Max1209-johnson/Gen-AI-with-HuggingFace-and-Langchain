# -*- coding: utf-8 -*-
"""1- Getting started with HiggingFace and Langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PviOsF6ktM0nAYgAZTOsw0urJXM_xVdx
"""

!pip install langchain-huggingface

!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install bitsandbytes
!pip install langchain

##Environment Secret Keys

from google.colab import userdata
sec_key = userdata.get("HF_TOKEN")
print(sec_key)

"""HuggingFace EndPoint"""

from langchain_huggingface import HuggingFaceEndpoint

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = sec_key

repo_id="mistralai/Mistral-7B-Instruct-v0.2"
llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.5,model_kwargs={"max_length":200,"token":sec_key})
print(llm)

llm.invoke("what is machine learning")

repo_id="mistralai/Mistral-7B-Instruct-v0.2"
llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.5,model_kwargs={"max_length":200,"token":sec_key})
print(llm.invoke("what is Generative AI?"))

from langchain import PromptTemplate,LLMChain
question="who won the cricket worldcup in 2011?"

template="""Question:{question}

Answer: Let's think step by step."""
prompt= PromptTemplate(template=template,input_variables=["question"])


final_chain=prompt|llm

print(final_chain.invoke(question))

!pip install transformers

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id="gpt2"
model=AutoModelForCausalLM.from_pretrained(model_id)
tokenizer=AutoTokenizer.from_pretrained(model_id)

"""HuggingFace Pipeline"""

# Use HuggingFace wiTH GPU

cpu_llm = HuggingFacePipeline.from_model_id(

    model_id="gpt2",
    task="text-generation",
    device=0,
    pipeline_kwargs={"max_new_tokens":100},
)

from langchain_core.prompts import prompt
template = """Question:{question}

Answer: Let's think step by step."""

prompt = PromptTemplate.from_template(template)